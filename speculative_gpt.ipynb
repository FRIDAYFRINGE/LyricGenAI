{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import time\n",
    "import tiktoken\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Model architectures \n",
    "\n",
    "# attn-> concat-> linear\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)x`\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention # (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# feedforward network: two linear maps with a GELU in between\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# transformer block\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# Configuration classes\n",
    "@dataclass\n",
    "class GPTConfig124M:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig19M:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 8\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 256\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        #weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        #init paramsxxxxxxxxxxxxxxxxxxx\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device= 'cuda' if torch.cuda.is_available() else \"cpu\") # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daeecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice : use of decorator might results in lose of metadata, @functools.wraps() -> to preserve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df4ed83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeculativeDecoder:\n",
    "    def __init__(self, draft_model, target_model):\n",
    "        self.draft_model = draft_model.to(device)\n",
    "        self.target_model = target_model.to(device)\n",
    "        self.draft_model.eval()\n",
    "        self.target_model.eval()\n",
    "    \n",
    "    def sample_token(self, logits, temperature=1.0, top_k=90):\n",
    "        \"\"\"Sample token using top-k sampling like your original code.\"\"\"\n",
    "        if temperature == 0:\n",
    "            return torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        if top_k > 0:\n",
    "            topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1)\n",
    "            ix = torch.multinomial(topk_probs, 1)\n",
    "            return torch.gather(topk_indices, -1, ix).item()\n",
    "        else:\n",
    "            return torch.multinomial(probs, 1).item()\n",
    "    \n",
    "    def draft_decode(self, input_ids, num_draft_tokens=4, temperature=1.0, top_k=90):\n",
    "        \"\"\"Generate draft tokens using the smaller model.\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_logits = []\n",
    "        current_ids = input_ids.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_draft_tokens):\n",
    "                logits, _ = self.draft_model(current_ids)\n",
    "                last_logits = logits[:, -1, :]\n",
    "                \n",
    "                next_token = self.sample_token(last_logits[0], temperature, top_k)\n",
    "                draft_tokens.append(next_token)\n",
    "                draft_logits.append(last_logits[0])\n",
    "                \n",
    "                current_ids = torch.cat([current_ids, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "        \n",
    "        return draft_tokens, torch.stack(draft_logits)\n",
    "    \n",
    "    def verify_and_correct(self, input_ids, draft_tokens, draft_logits, temperature=1.0, top_k=90):\n",
    "        \"\"\"Verify draft tokens with target model and apply rejection sampling.\"\"\"\n",
    "        extended_ids = torch.cat([\n",
    "            input_ids, \n",
    "            torch.tensor([draft_tokens], device=device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_logits, _ = self.target_model(extended_ids)\n",
    "            # FIX: Correct the logits alignment\n",
    "            start_pos = len(input_ids[0]) - 1  # Position that predicts first draft token\n",
    "            end_pos = start_pos + len(draft_tokens)\n",
    "            target_logits_for_draft = target_logits[0, start_pos:end_pos]\n",
    "        accepted_tokens = []\n",
    "        \n",
    "        for i, (draft_token, draft_logit, target_logit) in enumerate(\n",
    "            zip(draft_tokens, draft_logits, target_logits_for_draft)\n",
    "        ):\n",
    "            # Calculate acceptance probability\n",
    "            draft_prob = F.softmax(draft_logit / temperature, dim=-1)\n",
    "            target_prob = F.softmax(target_logit / temperature, dim=-1)\n",
    "            \n",
    "            acceptance_prob = torch.min(\n",
    "                torch.tensor(1.0, device=device),\n",
    "                target_prob[draft_token] / (draft_prob[draft_token] + 1e-10)\n",
    "            )\n",
    "            \n",
    "            if torch.rand(1, device=device) < acceptance_prob:\n",
    "                accepted_tokens.append(draft_token)\n",
    "            else:\n",
    "                # Rejection: sample from corrected distribution\n",
    "                corrected_prob = torch.clamp(target_prob - draft_prob, min=0)\n",
    "                corrected_prob = corrected_prob / (corrected_prob.sum() + 1e-10)\n",
    "                \n",
    "                if corrected_prob.sum() > 1e-10:\n",
    "                    if top_k > 0:\n",
    "                        topk_probs, topk_indices = torch.topk(corrected_prob, min(top_k, corrected_prob.shape[0]))\n",
    "                        ix = torch.multinomial(topk_probs, 1)\n",
    "                        corrected_token = torch.gather(topk_indices, -1, ix).item()\n",
    "                    else:\n",
    "                        corrected_token = torch.multinomial(corrected_prob, 1).item()\n",
    "                    accepted_tokens.append(corrected_token)\n",
    "                else:\n",
    "                    # Fallback to target model sampling\n",
    "                    corrected_token = self.sample_token(target_logit, temperature, top_k)\n",
    "                    accepted_tokens.append(corrected_token)\n",
    "                \n",
    "                break  # Stop after first rejection\n",
    "        \n",
    "        return accepted_tokens, len(accepted_tokens)\n",
    "    \n",
    "    def generate(self, input_ids, max_length=100, num_draft_tokens=4, temperature=1.0, top_k=90):\n",
    "        \"\"\"Generate text using speculative decoding.\"\"\"\n",
    "        generated_ids = input_ids.clone()\n",
    "        total_draft_tokens = 0\n",
    "        total_accepted_tokens = 0\n",
    "        num_iterations = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        while generated_ids.shape[1] < max_length:\n",
    "            num_iterations += 1\n",
    "            \n",
    "            # Generate draft tokens\n",
    "            draft_tokens, draft_logits = self.draft_decode(\n",
    "                generated_ids, num_draft_tokens, temperature, top_k\n",
    "            )\n",
    "            total_draft_tokens += len(draft_tokens)\n",
    "            \n",
    "            # Verify and correct with target model\n",
    "            accepted_tokens, num_accepted = self.verify_and_correct(\n",
    "                generated_ids, draft_tokens, draft_logits, temperature, top_k\n",
    "            )\n",
    "            total_accepted_tokens += num_accepted\n",
    "            \n",
    "            # Update generated sequence\n",
    "            if accepted_tokens:\n",
    "                new_tokens = torch.tensor([accepted_tokens], device=device)\n",
    "                generated_ids = torch.cat([generated_ids, new_tokens], dim=1)\n",
    "            \n",
    "            if generated_ids.shape[1] >= max_length:\n",
    "                break\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        acceptance_rate = total_accepted_tokens / total_draft_tokens if total_draft_tokens > 0 else 0\n",
    "        tokens_per_second = (generated_ids.shape[1] - input_ids.shape[1]) / (end_time - start_time)\n",
    "        \n",
    "        stats = {\n",
    "            'total_iterations': num_iterations,\n",
    "            'total_draft_tokens': total_draft_tokens,\n",
    "            'total_accepted_tokens': total_accepted_tokens,\n",
    "            'acceptance_rate': acceptance_rate,\n",
    "            'tokens_per_second': tokens_per_second,\n",
    "            'generation_time': end_time - start_time\n",
    "        }\n",
    "        \n",
    "        return generated_ids, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca42397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 124M parameter model...\n",
      "Loading 19M parameter model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_models():\n",
    "    \"\"\"Load both models.\"\"\"\n",
    "    print(\"Loading 124M parameter model...\")\n",
    "    target_model = GPT(GPTConfig124M())\n",
    "    checkpoint_124m = torch.load(\"124mpara.pth\", map_location='cpu', weights_only=True)\n",
    "    # this is cause of '_orig_mod' wrapper caused by dynamo and torch compile during training\n",
    "    checkpoint_124m = {k.replace('_orig_mod.', ''): v for k, v in checkpoint_124m.items()}\n",
    "    target_model.load_state_dict(checkpoint_124m)\n",
    "    print(\"Loading 19M parameter model...\")\n",
    "    draft_model = GPT(GPTConfig19M())\n",
    "    \n",
    "    checkpoint_19m = torch.load(\"19M.pth\", map_location='cpu', weights_only=True)\n",
    "    draft_model.load_state_dict(checkpoint_19m)\n",
    "\n",
    "    return draft_model, target_model\n",
    "\n",
    "def regular_generation(model, input_ids, max_length=100, temperature=1.0, top_k=90):\n",
    "    \"\"\"Regular generation for comparison.\"\"\"\n",
    "    model.eval()\n",
    "    xgen = input_ids.clone()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while xgen.size(1) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(xgen)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            \n",
    "            if top_k > 0:\n",
    "                topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1)\n",
    "                ix = torch.multinomial(topk_probs, 1)\n",
    "                xcol = torch.gather(topk_indices, -1, ix)\n",
    "            else:\n",
    "                xcol = torch.multinomial(probs, 1)\n",
    "            \n",
    "            xgen = torch.cat((xgen, xcol), dim=1)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    tokens_per_second = (xgen.shape[1] - input_ids.shape[1]) / (end_time - start_time)\n",
    "    \n",
    "    return xgen, {'tokens_per_second': tokens_per_second, 'generation_time': end_time - start_time}\n",
    "\n",
    "# Load models\n",
    "draft_model, target_model = load_models()\n",
    "# Initialize speculative decoder\n",
    "decoder = SpeculativeDecoder(draft_model, target_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59635e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: love in the air\n",
      "Input tokens: 4\n",
      "--------------------------------------------------\n",
      "Regular generation (124M model only):\n",
      "Generated text: love in the air  \n",
      "When you know it all begins  \n",
      "Baby it's in your ears  \n",
      "You must try to help those in your mind  \n",
      "Tonight  \n",
      "I dream so far away  \n",
      "I'd be so in the shadows  \n",
      "I'd always end behind you and have to decide  \n",
      "But somehow that it seems you would see me  \n",
      "Baby it's in your veins  \n",
      "But how can we part love another\n",
      "Speed: 7.58 tokens/sec\n",
      "Time: 12.67 seconds\n",
      "\n",
      "Speculative decoding (19M draft + 124M target):\n",
      "Generated text: love in the air  \n",
      "It never gets colder  \n",
      "Maybe I can't let go now  \n",
      "And never leave, never let go  \n",
      "If I could be, never let go  \n",
      "To the end if time has come true come true come true  \n",
      "Come closer don't break a little closer  \n",
      "Well I won't let go  \n",
      "If I could be I would be here once again  \n",
      "  \n",
      "If I could be\n",
      "Speed: 9.59 tokens/sec\n",
      "Time: 10.01 seconds\n",
      "Acceptance rate: 0.38\n",
      "Speedup: 1.26x\n"
     ]
    }
   ],
   "source": [
    "prompt = \"love in the air\"\n",
    "tokens = enc.encode(prompt)\n",
    "idx = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Input tokens: {len(tokens)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# params\n",
    "max_length = 100\n",
    "temperature = 1.2\n",
    "top_k = 90\n",
    "num_draft_tokens = 3\n",
    "\n",
    "# Regular generation with target model\n",
    "print(\"Regular generation (124M model only):\")\n",
    "regular_ids, regular_stats = regular_generation(\n",
    "    target_model, idx, max_length, temperature, top_k\n",
    ")\n",
    "regular_text = enc.decode(regular_ids[0].tolist())\n",
    "print(f\"Generated text: {regular_text}\")\n",
    "print(f\"Speed: {regular_stats['tokens_per_second']:.2f} tokens/sec\")\n",
    "print(f\"Time: {regular_stats['generation_time']:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# Speculative decoding\n",
    "print(\"Speculative decoding (19M draft + 124M target):\")\n",
    "spec_ids, spec_stats = decoder.generate(\n",
    "    idx, max_length, num_draft_tokens, temperature, top_k\n",
    ")\n",
    "spec_text = enc.decode(spec_ids[0].tolist())\n",
    "print(f\"Generated text: {spec_text}\")\n",
    "print(f\"Speed: {spec_stats['tokens_per_second']:.2f} tokens/sec\")\n",
    "print(f\"Time: {spec_stats['generation_time']:.2f} seconds\")\n",
    "print(f\"Acceptance rate: {spec_stats['acceptance_rate']:.2f}\")\n",
    "print(f\"Speedup: {spec_stats['tokens_per_second'] / regular_stats['tokens_per_second']:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "223fe82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 25\n",
      "Total draft tokens: 250\n",
      "Total accepted tokens: 96\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total iterations: {spec_stats['total_iterations']}\")\n",
    "print(f\"Total draft tokens: {spec_stats['total_draft_tokens']}\")\n",
    "print(f\"Total accepted tokens: {spec_stats['total_accepted_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ff7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
