# ğŸµ Sequence Generation Model  (LyricGenAI)

This repository contains the code and resources for training and generating sequences using a deep learning model.  

---

## ğŸš€ model architecture 

![{A9212516-5CBE-4370-9A46-36A66691C7FA}](https://github.com/user-attachments/assets/7e4b343b-39c4-4248-980f-3320a94f5dcd)



## ğŸ“‚ Repository Structure  
- **`model.py`** â†’ Code for training the model.  
- **`dataset`** â†’ Contains the dataset link used for training.  
- **`model_paras`** â†’ ğŸ¤— link to the trained model (`model.pth`).  
- **`play.ipynb`** â†’ Jupyter Notebook to load the trained model and generate sequences.  
---

## ğŸš€ Getting Started  

1ï¸âƒ£ Clone the Repository  

    git clone https://github.com/FRIDAYFRINGE/LyricGenAI.git
    cd LyricGenAI

2ï¸âƒ£ Download the Trained Model
Since GitHub has file size limits, download the model (model.pth) from ğŸ¤—:[Download model.pth](https://huggingface.co/spaces/fridayfringe/lgai/tree/main)

    After downloading, place model.pth in the same directory as play.ipynb.

3ï¸âƒ£ Install Dependencies

    !pip install -r requirements.txt

4ï¸âƒ£ Train the Model
To train the model from scratch, run:

    python model.py


5ï¸âƒ£ Run Inference
Open the Jupyter Notebook:
    
    jupyter notebook play.ipynb



ğŸ§  Model Details
Architecture: Transformer-based GPT model
Total Parameters: 19,446,528 (20M)
Embedding Size: 256
Number of Layers: 8
Number of Heads: 8
Max Sequence Length: 1024



ğŸ”§ Example: Loading the Model-
      
    # Load Weights
    model.load_state_dict(torch.load("model.pth", map_location=torch.device('gpu')))
    model.eval()



ğŸ“Œ Dataset
The dataset used for training can be found here:
ğŸ“‚ [Dataset Link](https://www.kaggle.com/datasets/vatsalmavani/spotify-dataset)

ğŸ“œ License
This project is licensed under the MIT License - see the LICENSE file for details.






